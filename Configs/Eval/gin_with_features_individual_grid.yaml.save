epochs: [100]
min_lr: [1e-5]
virtual_node: [0]
lr_schedule_patience: [5]
tracking: [0]

model: ["GIN"]
batch_size: [32]
emb_dim: [64, 256]
drop_out: [0, 0.5]
num_layer: [1, 2, 3, 4, 5]
pooling: ["sum", "mean"]
lr:  [0.001]
num_mlp_layers: [2]
lr_scheduler: ["ReduceLROnPlateau"]
lr_scheduler_decay_rate: [0.5]
